---
title: "[Paper Review]ADD-IT"
excerpt: "Paper Review"


categories:
 - paper
tags:
  - paper
  - diffusion model
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---
# ADD-IT: TRAINING-FREE OBJECT INSERTION IN IMAGES WITH PRETRAINED DIFFUSION MODELS

## 📚 소개
**ADD-IT**은 텍스트 지시를 기반으로 훈련 없이 이미지에 객체를 삽입하는 복잡한 작업을 다루며, 사전 학습된 텍스트-이미지 확산 모델을 활용하여 최첨단 결과를 달성함. 이 모델은 자연스러운 객체 삽입을 보장하기 위해 혁신적인 메커니즘을 도입함.

주요 동기는 다음과 같음:
- 구조적 및 시각적 일관성 유지.
- 추가된 객체가 맥락에 맞지 않는 기존 방법의 한계 극복.
- 객체 배치의 적합성을 측정하기 위한 새로운 평가 기준인 **Additing Affordance Benchmark** 제안.

---

## 🔑 주요 기여
1. **훈련이 필요 없는 접근법**:  
   - 추가적인 태스크별 파인튜닝 없이 수행 가능.  
   - FLUX와 같은 사전 학습된 확산 모델을 기반으로 설계됨.  

2. **혁신적인 메커니즘**:
   - **가중 확장 자기-어텐션(Weighted Extended Self-Attention)**: 소스 이미지, 텍스트 프롬프트, 타겟 이미지의 기여도를 균형있게 조정함.  
   - **구조 전이(Structure Transfer)**: 추가된 객체가 원본 장면과 조화를 이루도록 보장함.  
   - **주제 기반 잠재 블렌딩(Subject-Guided Latent Blending)**: 그림자 및 반사와 같은 세부 사항을 유지함.  

3. **새로운 벤치마크**:  
   - **Additing Affordance Benchmark**: 객체 배치의 적합성을 평가하기 위한 바운딩 박스와 탐지 지표를 포함함.  

---

## 📖 관련 연구 (Related Work)

### 1. 텍스트-이미지 확산 모델을 활용한 편집  

#### Prompt-to-Prompt (Hertz et al., 2022)  
**Prompt-to-Prompt**는 텍스트 기반 이미지 편집에서 소스 이미지의 구조적 특징을 유지하기 위해 어텐션 맵을 활용한 방법론임. 이 기술은 사전 학습된 텍스트-이미지 확산 모델에서 텍스트 프롬프트를 수정해 타겟 이미지를 생성하되, 소스 이미지의 시각적 정보를 보존함. 특히, 어텐션 레이어의 수정만으로 이미지 구조를 조정하지 않고도 의미 있는 변화를 유도함.   

- **핵심 기여**: 어텐션 맵 주입을 통해 소스 이미지의 구조적 특징을 타겟 이미지에 효과적으로 전달.  
- **ADD-IT과의 연관성**: ADD-IT의 **가중 확장 자기-어텐션(Weighted Extended Self-Attention)**은 Prompt-to-Prompt에서 제안한 어텐션 주입 방식을 발전시켜, 텍스트, 소스 이미지, 타겟 이미지 간의 균형을 조정함.  
- [논문 링크](https://arxiv.org/abs/2208.01626)  

#### SDEdit (Meng et al., 2022)  
**SDEdit**는 기존 이미지에 텍스트 지시를 기반으로 노이즈를 추가한 후, 점진적으로 디노이징 과정을 거쳐 텍스트와 일치하는 이미지를 생성하는 방법임. 이 접근법은 생성된 이미지가 원본 이미지의 주요 구조를 유지하면서도 새로운 텍스트 정보를 반영할 수 있도록 설계됨.  

- **핵심 기여**: 이미지의 구조 보존과 텍스트 적합성을 동시에 달성하기 위해 노이즈-디노이징 사이클을 활용.  
- **ADD-IT과의 연관성**: ADD-IT의 **구조 전이(Structure Transfer)**는 SDEdit의 노이즈 및 디노이징 과정에서 영감을 받아, 원본 이미지의 구조를 더욱 정교하게 유지하며 객체 삽입을 수행함.  
- [논문 링크](https://arxiv.org/abs/2108.01073)  

#### Blended Diffusion (Avrahami et al., 2022)  
**Blended Diffusion**은 이미지를 생성하는 동안 편집 영역(삽입된 객체와 그 주변 영역)과 원본 이미지의 잔여 영역을 자연스럽게 통합하는 기술임. 이는 마스크 기반 접근법을 활용하여 두 영역 사이의 이질감을 최소화하고, 객체 삽입 시의 시각적 품질을 크게 향상시킴.  

- **핵심 기여**: 생성 과정에서 블렌딩 기법을 사용해 삽입 객체와 원본 이미지 간의 시각적 일관성을 확보.  
- **ADD-IT과의 연관성**: ADD-IT의 **주제 기반 잠재 블렌딩(Subject-Guided Latent Blending)**은 Blended Diffusion의 블렌딩 아이디어를 확장하여, 객체 삽입뿐만 아니라 그림자 및 텍스처와 같은 세부 정보도 자연스럽게 통합함.  
- [논문 링크](https://arxiv.org/abs/2110.11778)  

### 2. 명령어 기반 이미지 편집  

#### InstructPix2Pix (Brooks et al., 2023)  
**InstructPix2Pix**는 텍스트 지시에 따라 이미지를 편집하는 명령어 기반 이미지 편집 모델임. 이 모델은 대규모 합성 데이터셋으로 학습된 사전 학습된 확산 모델을 기반으로 하며, 새로운 프롬프트를 통해 이미지의 특정 부분을 수정할 수 있음.  

- **핵심 기여**: 텍스트 지시를 이해하고, 이를 기반으로 이미지를 유연하게 편집할 수 있는 모델을 개발.  
- **ADD-IT과의 연관성**: ADD-IT과 마찬가지로 텍스트 지시 기반 접근을 활용하지만, InstructPix2Pix는 추가 학습이 필요했던 반면, ADD-IT은 비훈련 방식으로 차별화됨.  
- [논문 링크](https://arxiv.org/abs/2211.09800)  

#### MagicBrush (Zhang et al., 2023)  
**MagicBrush**는 InstructPix2Pix의 한계를 극복하기 위해 수동 주석 데이터셋으로 모델을 파인튜닝하여 더 정밀하고 자연스러운 결과를 제공함. 특히, 이미지 내 특정 영역을 선택적으로 수정하는 데 중점을 둠.  

- **핵심 기여**: 기존 명령어 기반 편집 모델의 성능을 개선하기 위해 사용자 주석 데이터셋 활용.  
- **ADD-IT과의 연관성**: MagicBrush는 InstructPix2Pix의 발전된 버전으로 ADD-IT의 목표와 유사한 고품질 편집을 지향함.  
- [논문 링크](https://arxiv.org/abs/2302.01560)  

---

## 🖼️ 방법론

### 1. 가중 확장 자기-어텐션  
ADD-IT은 확산 모델의 어텐션 메커니즘을 확장하여 다음 세 가지 주요 소스를 포함함:  
- **소스 이미지 $$ ((X_{source})) $$**  
- **텍스트 프롬프트 $$ ((P_{target})) $$**  
- **타겟 이미지 $$ ((X_{target})) $$**  

어텐션 공식은 다음과 같음:  

$$ A = \text{softmax}\left([Q_p, Q_{target}][\gamma_s \cdot K_{source}, \gamma_p \cdot K_p, \gamma_t \cdot K_{target}]^T / \sqrt{d_k}\right) $$   

여기서 $$ (\gamma_s, \gamma_p, \gamma_t) $$는 각각 소스, 프롬프트, 타겟의 기여도를 조정하는 가중치를 나타냄.  

### 2. 구조 전이 (Structure Transfer)  
추가된 객체가 원본 장면의 구조와 조화를 이루도록 보장하기 위해 다음을 수행함:  
- 소스 잠재 표현에 노이즈를 추가:  

$$ X_t = (1 - \sigma_t) \cdot x_0 + \sigma_t \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I) $$  

- 점진적인 디노이징 과정을 통해 구조적 정렬을 유지하면서 객체 추가를 가능하게 함.  

### 3. 주제 기반 잠재 블렌딩 (Subject-Guided Latent Blending)  
이 단계는 소스와 타겟 잠재 표현을 마스크 \(M\)를 사용하여 블렌딩함:  

$$ Z_{target} = M \odot Z_{target} + (1 - M) \odot Z_{source} $$  

- SAM-2 분할 모델과 동적 임계값을 사용하여 마스크를 생성 및 정제함.  

---

## 🧪 실험 결과

### 벤치마크
1. **Emu-Edit Benchmark**: 실제 이미지에서 객체 추가를 평가함.  
   - ADD-IT은 CLIP 기반 메트릭과 인간 평가에서 모든 방법을 능가함.  

2. **Additing Affordance Benchmark**: 객체 배치 적합성을 측정함.  
   - ADD-IT은 **82.8%의 적합성 점수**를 기록하며, 다음으로 높은 점수(47%)를 크게 초과함.  

### 결과  
| **방법**          | **적합성 점수** |
|:---------------:|:----------:|
| InstructPix2Pix | 27.6%      |
| MagicBrush      | 41.8%      |
| **ADD-IT**      | **82.8%**  |

### 사용자 연구  
- 인간 평가에서 ADD-IT은 **80% 이상의 사례에서 선호**되며, 자연스럽고 맥락 인식적인 객체 삽입을 강조함.  

---

## 🎨 응용 분야  
1. **콘텐츠 제작**: 텍스트 기반의 반복적인 편집으로 복잡한 장면을 생성할 수 있음.  
2. **자율 주행**: 새로운 시나리오를 시뮬레이션하여 인식 시스템을 훈련시킬 수 있음.  
3. **합성 데이터**: 다운스트림 태스크를 위한 데이터셋을 확장할 수 있음.  

---

## 🚧 한계점  
1. **데이터 편향**: 사전 학습된 모델의 편향으로 인해 복잡한 장면에서 오류가 발생할 수 있음.  
2. **프롬프트 민감성**: 정확한 결과를 위해 상세한 프롬프트가 필요함.  
3. **실제 이미지 성능**: 역변환 문제로 인해 생성된 이미지보다 약간 낮은 성능을 보임.  

---

## 📈 결론
ADD-IT은 훈련 없이 텍스트 기반 이미지 편집에서 자연스러운 객체 삽입을 가능하게 하며, 혁신적인 메커니즘과 벤치마크를 통해 복잡한 장면에서 객체 배치를 평가하는 새로운 기준을 제시함.  

---

## 🔗 참고 자료
전체 논문은 [여기](https://research.nvidia.com/labs/par/addit/)에서 확인할 수 있음.

---


