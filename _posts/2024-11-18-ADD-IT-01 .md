---
title: "[Paper Review]ADD-IT"
excerpt: "Paper Review"


categories:
 - paper
tags:
  - paper
  - diffusion model
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---
# ADD-IT: 훈련 없이 사전 학습된 확산 모델을 활용한 객체 삽입

## 📚 소개
**ADD-IT**은 텍스트 지시를 기반으로 훈련 없이 이미지에 객체를 삽입하는 복잡한 작업을 다루며, 사전 학습된 텍스트-이미지 확산 모델을 활용하여 최첨단 결과를 달성함. 이 모델은 자연스러운 객체 삽입을 보장하기 위해 혁신적인 메커니즘을 도입함.

주요 동기는 다음과 같음:
- 구조적 및 시각적 일관성 유지.
- 추가된 객체가 맥락에 맞지 않는 기존 방법의 한계 극복.
- 객체 배치의 적합성을 측정하기 위한 새로운 평가 기준인 **Additing Affordance Benchmark** 제안.

---

## 🔑 주요 기여
1. **훈련이 필요 없는 접근법**:  
   - 추가적인 태스크별 파인튜닝 없이 수행 가능.  
   - FLUX와 같은 사전 학습된 확산 모델을 기반으로 설계됨.  

2. **혁신적인 메커니즘**:
   - **가중 확장 자기-어텐션(Weighted Extended Self-Attention)**: 소스 이미지, 텍스트 프롬프트, 타겟 이미지의 기여도를 균형있게 조정함.  
   - **구조 전이(Structure Transfer)**: 추가된 객체가 원본 장면과 조화를 이루도록 보장함.  
   - **주제 기반 잠재 블렌딩(Subject-Guided Latent Blending)**: 그림자 및 반사와 같은 세부 사항을 유지함.  

3. **새로운 벤치마크**:  
   - **Additing Affordance Benchmark**: 객체 배치의 적합성을 평가하기 위한 바운딩 박스와 탐지 지표를 포함함.  

---

## 🖼️ 방법론

### 1. 가중 확장 자기-어텐션  
ADD-IT은 확산 모델의 어텐션 메커니즘을 확장하여 다음 세 가지 주요 소스를 포함함:  
- **소스 이미지 $$ ((X_{source})) $$**  
- **텍스트 프롬프트 $$ ((P_{target})) $$**  
- **타겟 이미지 $$ ((X_{target})) $$**  

어텐션 공식은 다음과 같음:  
$$ A = \text{softmax}\left([Q_p, Q_{target}][\gamma_s \cdot K_{source}, \gamma_p \cdot K_p, \gamma_t \cdot K_{target}]^T / \sqrt{d_k}\right) $$  
여기서 $$ (\gamma_s, \gamma_p, \gamma_t) $$는 각각 소스, 프롬프트, 타겟의 기여도를 조정하는 가중치를 나타냄.  

### 2. 구조 전이 (Structure Transfer)  
추가된 객체가 원본 장면의 구조와 조화를 이루도록 보장하기 위해 다음을 수행함:  
- 소스 잠재 표현에 노이즈를 추가:  

$$ X_t = (1 - \sigma_t) \cdot x_0 + \sigma_t \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I) $$  

- 점진적인 디노이징 과정을 통해 구조적 정렬을 유지하면서 객체 추가를 가능하게 함.  

### 3. 주제 기반 잠재 블렌딩 (Subject-Guided Latent Blending)  
이 단계는 소스와 타겟 잠재 표현을 마스크 \(M\)를 사용하여 블렌딩함:  

$$ Z_{target} = M \odot Z_{target} + (1 - M) \odot Z_{source} $$  

- SAM-2 분할 모델과 동적 임계값을 사용하여 마스크를 생성 및 정제함.  

---

## 🧪 실험 결과

### 벤치마크
1. **Emu-Edit Benchmark**: 실제 이미지에서 객체 추가를 평가함.  
   - ADD-IT은 CLIP 기반 메트릭과 인간 평가에서 모든 방법을 능가함.  

2. **Additing Affordance Benchmark**: 객체 배치 적합성을 측정함.  
   - ADD-IT은 **82.8%의 적합성 점수**를 기록하며, 다음으로 높은 점수(47%)를 크게 초과함.  

### 결과
| **방법**              | **적합성 점수** |
|-----------------------|----------------|
| InstructPix2Pix       | 27.6%         |
| MagicBrush            | 41.8%         |
| **ADD-IT**            | **82.8%**     |

### 사용자 연구  
- 인간 평가에서 ADD-IT은 **80% 이상의 사례에서 선호**되며, 자연스럽고 맥락 인식적인 객체 삽입을 강조함.  

---

## 🎨 응용 분야  
1. **콘텐츠 제작**: 텍스트 기반의 반복적인 편집으로 복잡한 장면을 생성할 수 있음.  
2. **자율 주행**: 새로운 시나리오를 시뮬레이션하여 인식 시스템을 훈련시킬 수 있음.  
3. **합성 데이터**: 다운스트림 태스크를 위한 데이터셋을 확장할 수 있음.  

---

## 🚧 한계점  
1. **데이터 편향**: 사전 학습된 모델의 편향으로 인해 복잡한 장면에서 오류가 발생할 수 있음.  
2. **프롬프트 민감성**: 정확한 결과를 위해 상세한 프롬프트가 필요함.  
3. **실제 이미지 성능**: 역변환 문제로 인해 생성된 이미지보다 약간 낮은 성능을 보임.  

---

## 📈 결론
ADD-IT은 훈련 없이 텍스트 기반 이미지 편집에서 자연스러운 객체 삽입을 가능하게 하며, 혁신적인 메커니즘과 벤치마크를 통해 복잡한 장면에서 객체 배치를 평가하는 새로운 기준을 제시함.  

---

## 🔗 참고 자료
전체 논문은 [여기](https://research.nvidia.com/labs/par/addit/)에서 확인할 수 있음.

---


