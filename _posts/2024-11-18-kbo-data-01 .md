---
title: "ADD-IT"
excerpt: "Paper Review"


categories:
 - paper
tags:
  - paper
  - diffusion model
search: true

# 목차
toc: true  
toc_sticky: true 

use_math: true
---
# ADD-IT: Training-Free Object Insertion in Images with Pretrained Diffusion Models

## 1. Introduction
ADD-IT는 텍스트 기반 객체 삽입이라는 이미지 편집 문제를 해결하기 위해 제안된 **훈련 없는 접근법**임. 이 방법은 원본 장면의 구조와 디테일을 유지하면서 텍스트 프롬프트에 따라 새로운 객체를 자연스럽게 삽입함. 기존 방법들은 다음과 같은 문제를 겪음:
- 텍스트 프롬프트와 원본 이미지 간 정렬 실패
- 부자연스러운 객체 배치 및 왜곡

AdD-IT는 사전 학습된 텍스트-이미지 확산 모델의 **확장된 주의(attention) 메커니즘**을 활용하여 이러한 한계를 극복함. 기존 연구는 객체 삽입 시 **affordance**(사람과 객체 간 상호작용에 대한 심층적 의미론적 지식)를 고려하지 못해 실패했으며, AdD-IT는 이를 해결하기 위해 다음과 같은 접근 방식을 제안함:
1. 확산 모델의 멀티모달 주의 메커니즘을 확장하여 원본 이미지, 텍스트 프롬프트, 생성된 이미지 정보를 통합함.
2. 구조 전송(structure transfer)과 잠재 혼합(latent blending)을 통해 장면 일관성과 세부 사항을 유지함.
3. 추가적인 훈련 없이 기존 모델의 지식을 활용하여 새로운 객체를 자연스럽게 삽입함.

이 방법은 인간 평가에서 80% 이상의 선호도를 얻었으며, 자동화된 평가 기준에서도 기존 방법들을 능가하는 성능을 보였음.

---

## 2. Related Work

### 2.1 Object Placement and Insertion
객체 삽입은 전통적인 컴퓨터 그래픽스 방식부터 딥러닝 기반 생성 모델까지 발전해 왔으나, 현실적인 장면에서 자연스러운 배치를 보장하는 데 한계가 있었음.  
기존 연구들은 다음과 같은 접근 방식을 사용했음:
- **전통적 그래픽스 방식**: 수동으로 객체를 배치하거나 합성 데이터를 활용하여 배치 위치를 예측함.
- **딥러닝 기반 방식**: 생성 모델을 학습시켜 객체 배치를 자동화하려 했으나, 학습 데이터에 의존하여 일반화에 실패하거나 복잡한 장면에서 부자연스러운 결과를 생성했음.

### 2.2 Editing with Text-to-Image Diffusion Models
텍스트-이미지 확산 모델은 다양한 이미지 편집 작업에서 성공했으나, 객체 삽입 작업에서는 다음과 같은 문제를 겪었음:
- 원본 이미지와 텍스트 간 정렬 실패
- 삽입된 객체의 부자연스러운 외형
- 장면 일관성을 유지하지 못함

ADD-IT는 이러한 한계를 해결하기 위해 **추가 훈련 없이** 사전 학습된 모델의 잠재력을 활용하며, 멀티모달 주의 메커니즘과 구조 전송 기법을 결합하여 성능을 향상시킴.

---

## 3. Method

ADD-IT는 다음 세 가지 주요 구성 요소로 이루어져 있음:

### 3.1 Weighted Extended Self-Attention
확장된 주의 메커니즘을 통해 원본 이미지, 텍스트 프롬프트, 대상 이미지 간 균형을 유지함. 각 정보 소스의 기여도를 조정하기 위해 가중치 파라미터 \( \gamma_s, \gamma_p, \gamma_t \)를 도입함.

$$
A = \operatorname{softmax}\left(\left[Q_{p}, Q_{\text{target}}\right]\left[\gamma_{s} \cdot K_{\text{source}}, \gamma_{p} \cdot K_{p}, \gamma_{t} \cdot K_{\text{target}}\right]^{\top} / \sqrt{d_k}\right)
$$


$$
h = A \cdot \left[V_{\text{source}}, V_{p}, V_{\text{target}}\right]
$$


여기서 \( Q, K, V \)는 각각 Query, Key, Value를 나타내며, 소스 이미지와 텍스트 프롬프트 및 타겟 이미지에서 추출됨.

---

### 3.2 Structure Transfer
원본 이미지의 구조를 대상 이미지로 전달하여 장면 일관성을 유지함.

$$
X_t = (1 - \sigma_t) x_0 + \sigma_t \epsilon
$$


여기서 \( X_t \)는 노이즈가 추가된 잠재 공간(latent space)이며, \( x_0 \)는 원본 이미지, \( \epsilon \sim \mathcal{N}(0, I) \)는 가우시안 노이즈임.

---

### 3.3 Subject-Guided Latent Blending
삽입된 객체로 인해 영향을 받지 않은 원본 이미지의 세부 사항(예: 그림자 및 반사)을 보존하기 위한 기법임.

$$
Z_{\text{target}} = M \odot Z_{\text{target}} + (1 - M) \odot Z_{\text{source}}
$$


여기서 \( M \)은 마스크(mask)를 나타내며, \( Z_{\text{source}}, Z_{\text{target}} \)는 각각 소스와 타겟 잠재 공간임.

---

## 4. Experiments

### 4.1 Emu-Edit Benchmark
ADD-IT는 Emu-Edit Benchmark에서 기존 방법보다 우수한 성능을 보임.

#### CLIP$$_{\text{dir}}$$:
$$
\text{CLIP}_{\text{dir}} = 
\cos(\text{CLIP}(\text{Image}_{\text{before}}) - 
\text{CLIP}(\text{Image}_{\text{after}}), 
\text{CLIP}(\text{Text}_{\text{before}}) - 
\text{CLIP}(\text{Text}_{\text{after}})
)
$$


#### CLIP$$_{\text{out}}$$:
$$
\text{CLIP}_{\text{out}} = 
\cos(\text{CLIP}(\text{Image}_{\text{after}}), 
\text{CLIP}(\text{Text}_{\text{after}})
)
$$


ADD-IT는 CLIP 기반 메트릭(CLIP$$_{\text{dir}}, CLIP$$_{\text{out}}$$)에서 최고 성능을 기록했으며 인간 평가에서도 높은 선호도를 얻었음.

---

## 5. Analysis

ADD-IT의 주요 설계 요소를 분석한 결과:
1. **가중치 스케일링은 $\gamma_s$, $\gamma_p$, $\gamma_t$**은 소스와 대상 이미지 간 균형을 유지하며 객체 삽입과 장면 일관성을 동시에 확보할 수 있었음.
2. **구조 전송 및 잠재 혼합 단계**는 시각적 왜곡 없이 세부 사항을 보존하는 데 기여했음.

---

## 6. Limitations

ADD-IT는 강력한 성능을 보여주지만 몇 가지 한계가 있음:
1. 사전 학습된 모델의 편향(bias)을 계승할 가능성이 있음.
2. 실제 이미지에 대한 성능이 생성된 이미지보다 낮음.
3. 동일한 객체가 반복될 경우 추가적인 프롬프트 세부화가 필요함.

---

## 7. Conclusion

ADD-IT는 훈련 없이도 텍스트 프롬프트를 기반으로 객체를 자연스럽게 삽입할 수 있는 혁신적인 방법임. 새로운 평가 기준(Affordance Benchmark)을 제시하며 기존 방식보다 우수한 성능을 달성했음을 입증함.

특히 인간 평가에서 높은 선호도를 얻었으며, 자동화된 메트릭에서도 강력한 성능을 보여줌으로써 실제 응용 가능성을 입증하였음.

